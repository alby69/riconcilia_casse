import os
import re
from datetime import datetime
import argparse
import sys
import pandas as pd


def analizza_log(data_str=None, export_csv=False):
    """
    Analyzes log files for a specific date (or for today if not specified).

    Args:
        data_str (str, optional): The date to analyze in 'YYYY-MM-DD' format.
                                  If None, the current date is used.

        export_csv (bool, optional): If True, exports the results to a CSV file.
    This script has been adapted to the structure of the logs generated by batch.py.
    """
    log_dir = 'log'
    # Look for the log folder in the current directory or the one above (if moved to tools/)
    if not os.path.isdir(log_dir):
        if os.path.isdir(os.path.join('..', 'log')):
            log_dir = os.path.join('..', 'log')
        else:
            print(f"Error: Folder '{log_dir}' not found.")
            print("Make sure to run the script from the main project folder 'riconcilia_casse'.")
            return

    if data_str:
        try:
            # Validate the date format provided by the user
            datetime.strptime(data_str, '%Y-%m-%d')
            data_da_analizzare = data_str
        except ValueError:
            print(f"Error: Invalid date format '{data_str}'. Use YYYY-MM-DD format.", file=sys.stderr)
            return
    else:
        data_da_analizzare = datetime.now().strftime('%Y-%m-%d')
    risultati = []

    # Extended regex to capture all optimization parameters.
    # Use named groups (?P<name>...) to make the code more readable and robust.
    # Parameters are optional (.*?) to handle older logs that might not have them.
    block_regex = re.compile(
        r"File: (.*?)\n"  # Cattura il nome del file originale
        r".*?% Importo DARE utilizzato: (?P<perc_dare>\d+\.\d+)%\n"
        r".*?% Importo AVERE utilizzato: (?P<perc_avere>\d+\.\d+)%\n"
        r".*?Parametri Ottimali Usati:.*?\n"
        r".*?- giorni_finestra: (?P<giorni_finestra>\d+)\n"
        r".*?- max_combinazioni: (?P<max_combinazioni>\d+)\n"
        r".*?- giorni_finestra_residui: (?P<giorni_finestra_residui>\d+)\n"
        r".*?- soglia_residui: (?P<soglia_residui>[\d\.]+)\n"
        r".*?- sorting_strategy: (?P<sorting_strategy>\w+)\n"
        r".*?- search_direction: (?P<search_direction>\w+)\n"
        r".*?- tolleranza: (?P<tolleranza>[\d\.]+)",
        re.DOTALL  # Allows '.' to include newlines as well
    )

    # Log files are named with a timestamp, e.g., 2025-12-10_16-02-18_summary.log
    for filename in sorted(os.listdir(log_dir)):
        # Filter files for the target date
        if filename.endswith("_summary.log") and filename.startswith(data_da_analizzare):
            filepath = os.path.join(log_dir, filename)
            try:
                with open(filepath, 'r', encoding='utf-8') as f:
                    content = f.read()
                    
                    # Extract the time from the filename
                    try:
                        timestamp_str = filename.split('_summary.log')[0]
                        orario = datetime.strptime(timestamp_str, '%Y-%m-%d_%H-%M-%S').strftime('%H:%M:%S')
                    except ValueError:
                        orario = "N/D"
                    
                    # Find all result blocks in the log file content
                    matches = block_regex.finditer(content)
                    
                    for i, match in enumerate(matches):
                        original_filename = match.group(1)
                        match_dict = match.groupdict()

                        if original_filename and match_dict.get('perc_dare') and match_dict.get('perc_avere'):
                            perc_dare = float(match_dict['perc_dare'])
                            perc_avere = float(match_dict['perc_avere'])
                            risultati.append({
                                'orario': orario,
                                'original_filename': original_filename.strip(),
                                'perc_dare': perc_dare,
                                'perc_avere': perc_avere,
                                'delta': perc_dare - perc_avere,
                                # Add all other parameters, handling types
                                'giorni_finestra': int(match_dict.get('giorni_finestra', 0)),
                                'max_combinazioni': int(match_dict.get('max_combinazioni', 0)),
                                'tolleranza': float(match_dict.get('tolleranza', 0.0)),
                                'giorni_finestra_residui': int(match_dict.get('giorni_finestra_residui', 0)),
                                'soglia_residui': float(match_dict.get('soglia_residui', 0.0)),
                                'sorting_strategy': match_dict.get('sorting_strategy', 'N/D'),
                                'search_direction': match_dict.get('search_direction', 'N/D'),
                                'file_log': filename,
                                'run_index': i + 1 # To distinguish results within the same log
                            })

            except Exception as e:
                print(f"Error while reading file {filename}: {e}")

    if not risultati:
        print(f"No results found for date {data_da_analizzare}.")
        print(f"The script looks for files like '{data_da_analizzare}_*_summary.log' in the '{log_dir}' folder.")
        return

    # Sort results: primary key 'original_filename', secondary key 'orario'
    risultati.sort(key=lambda x: (x['original_filename'], x['orario']))

    # --- Print to console ---
    _stampa_tabella(risultati, data_da_analizzare)

    # --- CSV Export (if requested) ---
    if export_csv:
        _esporta_csv(risultati, data_da_analizzare)

def _stampa_tabella(risultati, data_da_analizzare):
    """Prints the formatted results in a table to the console."""
    # Print the results table
    print(f"--- Reconciliation Results Analysis for {data_da_analizzare} ---")
    # We adjust the width and columns for better readability
    header = f"{'Original File':<25} | {'Time':<10} | {'Window':<8} | {'Max Comb':<8} | {'Toler.':<7} | {'% DEBIT':<10} | {'% CREDIT':<10} | {'Delta %':<10} | {'Log File'}"
    print("-" * len(header))
    print(header)
    print("-" * len(header))

    # Group by filename for a clear display
    last_filename = None
    for res in risultati:
        if res['original_filename'] != last_filename and last_filename is not None:
            print("-" * len(header))
        
        # Show the filename only for the first row of its group
        filename_display = res['original_filename'] if res['original_filename'] != last_filename else ""
        
        print(f"{filename_display:<25} | {res['orario']:<10} | {res['giorni_finestra']:<8} | {res['max_combinazioni']:<8} | {res['tolleranza']:<7.2f} | {res['perc_dare']:>9.2f}% | {res['perc_avere']:>9.2f}% | {res['delta']:>+9.2f}% | {res['file_log']}")
        last_filename = res['original_filename']

    print("-" * len(header))

def _esporta_csv(risultati, data_da_analizzare):
    """Exports the results to a CSV file."""
    output_dir = 'output'
    os.makedirs(output_dir, exist_ok=True)
    csv_filename = os.path.join(output_dir, f"analisi_log_{data_da_analizzare}.csv")
    
    df = pd.DataFrame(risultati)
    df.to_csv(csv_filename, index=False, sep=';', decimal=',')
    print(f"\nâœ… Results successfully exported to: {csv_filename}")

if __name__ == "__main__":
    parser = argparse.ArgumentParser(
        description="Analyzes reconciliation logs for a specific date.",
        formatter_class=argparse.RawTextHelpFormatter
    )
    parser.add_argument('--data', help="The date to analyze in 'YYYY-MM-DD' format.\nIf omitted, analyzes today's logs.")
    parser.add_argument('--csv', action='store_true', help="Exports the output to a CSV file in the 'output' folder.")
    args = parser.parse_args()

    analizza_log(data_str=args.data, export_csv=args.csv)
